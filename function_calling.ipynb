{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored\n",
    "\n",
    "api_key=\"\"\n",
    "\n",
    "# GPT_MODEL_3 = \"gpt-3.5-turbo-0125\"\n",
    "GPT_MODEL_3 = \"gpt-3.5-turbo-1106\"\n",
    "GPT_MODEL_3_LONG = \"gpt-4-32k-0314\"\n",
    "GPT_MODEL_4 = \"gpt-4-0613\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(messages, functions, model, function_call):\n",
    "    try:\n",
    "        # messages = []\n",
    "        # messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "        # messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        # messages = [{'role': 'user', 'content': user_prompt}]\n",
    "        response = client.chat.completions.create(\n",
    "            model = model,\n",
    "            messages = messages,\n",
    "            functions = functions,\n",
    "            function_call = function_call\n",
    "        )\n",
    "        # return response.choices[0].message.function_call.arguments\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_calling.functions import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Chating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = []\n",
    "# messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "# messages.append({\"role\": \"user\", \"content\": \"What's the trend of NQ\"})\n",
    "# chat_response = get_openai_response(\n",
    "#     messages=messages, functions=tools, model=GPT_MODEL_3, tool_choice=\"auto\"\n",
    "# )\n",
    "# assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "# messages.append(assistant_message)\n",
    "# assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"symbol\": \"NQ\"\\n}', name='detect_trend'), tool_calls=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What is the trend of NQ stock?\"})\n",
    "chat_response = get_response(\n",
    "    messages, functions, GPT_MODEL_3, \"auto\"\n",
    "    # messages, functions=functions, tool_choice=\"auto\", model=GPT_MODEL_3\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='from datetime import datetime, timedelta\\n\\nend_date = datetime.now()\\nstart_date = end_date - timedelta(days=30)\\n\\ntimerange = [int(start_date.timestamp()), int(end_date.timestamp())]\\n\\n{\\n  \"symbol\": \"NQ\",\\n  \"timerange\": timerange\\n}', name='python'), tool_calls=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"over the last month\"})\n",
    "chat_response = get_response(\n",
    "    messages, functions, GPT_MODEL_3, \"auto\"\n",
    ")\n",
    "chat_response.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      4\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the trend of NQ stock over the last month?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 5\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m(\n\u001b[1;32m      6\u001b[0m     messages, functions, GPT_MODEL_3, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_response' is not defined"
     ]
    }
   ],
   "source": [
    "# in this cell we force the model to use get_n_day_weather_forecast\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What is the trend of NQ stock over the last month?\"})\n",
    "chat_response = get_response(\n",
    "    messages, functions, GPT_MODEL_3, \"auto\"\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-94AQRnMJlcCEgkbmiwEaNNj3EMvy0', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"symbol\": \"NQ\",\\n  \"timerange\": \"last month\"\\n}', name='detect_trend'), tool_calls=None))], created=1710780415, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=1439, total_tokens=1465))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"symbol\": \"NQ\",\n",
      "  \"timerange\": \"last month\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"What is the trend of NQ stock over the last month?\"\n",
    "\n",
    "output = get_response(user_prompt, functions, model=GPT_MODEL_3, function_call=\"auto\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in this cell we force the model to use show_trend\n",
    "# messages = []\n",
    "\n",
    "# messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "# messages.append({\"role\": \"user\", \"content\": \"Show me the trend of E-mini Nasdaq-100(NQ) over the past two weeks\"})\n",
    "\n",
    "# chat_response = chat_completion_request(\n",
    "#     messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"show_trend\"}}, model=GPT_MODEL_3\n",
    "# )\n",
    "\n",
    "# chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"symbol\": \"NQ\",\n",
      "  \"timerange\": \"1m\",\n",
      "  \"trend\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Show me the trend of NQ stock which is equal to one over the last month.\"\n",
    "\n",
    "output = get_response(user_prompt, functions, model=GPT_MODEL_3, function_call=\"auto\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in this cell we force the model to use order_entry_risk_manager\n",
    "# messages = []\n",
    "\n",
    "# messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "# messages.append({\"role\": \"user\", \"content\": \"Open a long position for me in E-mini Nasdaq-100(NQ) with order type of limit and price of 2.0 and size of 10 and stop loss of 5 and take profit of 20\"})\n",
    "\n",
    "# chat_response = chat_completion_request(\n",
    "#     messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"order_entry_risk_manager\"}}, model=GPT_MODEL_3\n",
    "# )\n",
    "\n",
    "# chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"symbol\": \"ES\",\n",
      "  \"direction\": \"long\",\n",
      "  \"order_type\": \"limit\",\n",
      "  \"price\": \"10.1 dollars\",\n",
      "  \"size\": 10,\n",
      "  \"sl\": \"1.2 dollars\",\n",
      "  \"tp\": \"5.78 dollars\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Would it be successful if I open a long position with the type of limit based on ES and price of 10.1 dollars and size of 10 and stop loss of 1.2 dollars and take profit of 5.78 dollars?\"\n",
    "\n",
    "output = get_response(user_prompt, functions, model=GPT_MODEL_3, function_call=\"auto\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Support and Resistance Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in this cell we force the model to use calculate_sr\n",
    "# messages = []\n",
    "\n",
    "# messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "# messages.append({\"role\": \"user\", \"content\": \"Calculate support and resistance levels of E-mini Nasdaq-100(NQ) for the past 2 days with the time frame of 2 hours\"})\n",
    "\n",
    "# chat_response = chat_completion_request(\n",
    "#     messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"calculate_sr\"}}, model=GPT_MODEL_3\n",
    "# )\n",
    "\n",
    "# chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"symbol\": \"RTY\",\n",
      "  \"lookback_days\": \"15\",\n",
      "  \"timeframe\": \"2d\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Calculate Support and Resistance Levels based on RTY by looking back upto 15 days and timeframe of two days\"\n",
    "\n",
    "output = get_response(user_prompt, functions, model=GPT_MODEL_3, function_call=\"auto\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in this cell we force the model to use news_analysis\n",
    "# messages = []\n",
    "\n",
    "# messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "# messages.append({\"role\": \"user\", \"content\": \"What effect would the news of the past 2 weeks of federal announcement would have on E-mini Nasdaq-100(NQ)\"})\n",
    "\n",
    "# chat_response = chat_completion_request(\n",
    "#     messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"news_analysis\"}}, model=GPT_MODEL_3\n",
    "# )\n",
    "\n",
    "# chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"symbol\": \"CL\",\n",
      "  \"timerange\": [2, 4],\n",
      "  \"news_type\": \"federal announcement\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Can you analyze the federal announcement news with time range of 2 and 4 days on CL?\"\n",
    "\n",
    "output = get_response(user_prompt, functions, model=GPT_MODEL_3, function_call=\"auto\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_samples = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in this cell we force the model to use news_analysis\n",
    "# messages = []\n",
    "\n",
    "# messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "# messages.append({\"role\": \"user\", \"content\": prompt_samples[0]})\n",
    "\n",
    "# chat_response = chat_completion_request(\n",
    "#     messages, tools=tools, tool_choice=tools, model=GPT_MODEL_3\n",
    "# )\n",
    "\n",
    "# chat_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sepehr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
